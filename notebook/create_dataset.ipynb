{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eb577b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/bo_ling/python39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "SCRIPT_DIR = os.path.dirname(os.path.abspath(\"...\"))\n",
    "sys.path.append(os.path.dirname(SCRIPT_DIR))\n",
    "from training.generate import (generate_response, load_model_tokenizer_for_generate, \n",
    "                               get_special_token_id, get_special_token_id)\n",
    "from training.transcript_trainer import PROMPT_FORMAT, create_data_set_from_json_list\n",
    "from datasets import Dataset, DatasetDict, load_dataset, load_from_disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "abeaf94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset generator/default to /home/prodadmin/.cache/huggingface/datasets/generator/default-71d75ecb741b2dcb/0.0.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generator downloaded and prepared to /home/prodadmin/.cache/huggingface/datasets/generator/default-71d75ecb741b2dcb/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                              \r"
     ]
    }
   ],
   "source": [
    "file_to_instruction_map = {\n",
    "    \"/opt/home/bo_ling/dataset/michelangelo_so_long.jsonl\": \"Please answer the following MA helpdesk questions:\",\n",
    "    \"/opt/home/bo_ling/dataset/docstrans.jsonl\": \"Please finish the following doc translation tasks:\",\n",
    "    \"/opt/home/bo_ling/dataset/eats_goldenset.jsonl\": \"Please answer Uber eats products relation questions:\"\n",
    "}\n",
    "json_file = \"/opt/home/bo_ling/dataset/eats_goldenset.jsonl\"\n",
    "dataset_file = json_file.replace(\".jsonl\", \".hf\")\n",
    "dataset = create_data_set_from_json_list(json_file,\n",
    "                                         file_to_instruction_map=file_to_instruction_map) \n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "dataset.save_to_disk(dataset_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4366efd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset generator/default to /home/prodadmin/.cache/huggingface/datasets/generator/default-539c4d3dc8c83bf1/0.0.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating train split: 0 examples [00:00, ? examples/s]\u001b[A\n",
      "Generating train split: 21405 examples [00:00, 213216.51 examples/s]\u001b[A\n",
      "Generating train split: 43000 examples [00:00, 211407.16 examples/s]\u001b[A\n",
      "Generating train split: 74780 examples [00:00, 210327.20 examples/s]\u001b[A\n",
      "Generating train split: 96000 examples [00:00, 207296.21 examples/s]\u001b[A\n",
      "Generating train split: 117000 examples [00:00, 206864.72 examples/s]\u001b[A\n",
      "Generating train split: 138000 examples [00:00, 205925.12 examples/s]\u001b[A\n",
      "Generating train split: 159000 examples [00:00, 205540.89 examples/s]\u001b[A\n",
      "Generating train split: 180000 examples [00:00, 205121.36 examples/s]\u001b[A\n",
      "Generating train split: 201000 examples [00:00, 205094.32 examples/s]\u001b[A\n",
      "Generating train split: 222000 examples [00:01, 206168.52 examples/s]\u001b[A\n",
      "Generating train split: 244000 examples [00:01, 208367.63 examples/s]\u001b[A\n",
      "Generating train split: 266000 examples [00:01, 209475.40 examples/s]\u001b[A\n",
      "Generating train split: 288000 examples [00:01, 210317.80 examples/s]\u001b[A\n",
      "Generating train split: 320000 examples [00:01, 210763.61 examples/s]\u001b[A\n",
      "Generating train split: 341191 examples [00:01, 211069.81 examples/s]\u001b[A\n",
      "Generating train split: 363000 examples [00:01, 212221.34 examples/s]\u001b[A\n",
      "Generating train split: 385000 examples [00:01, 213460.46 examples/s]\u001b[A\n",
      "Generating train split: 407000 examples [00:01, 214099.32 examples/s]\u001b[A\n",
      "Generating train split: 429000 examples [00:02, 214044.25 examples/s]\u001b[A\n",
      "Generating train split: 451000 examples [00:02, 214528.31 examples/s]\u001b[A\n",
      "Generating train split: 473000 examples [00:02, 214122.23 examples/s]\u001b[A\n",
      "Generating train split: 495032 examples [00:02, 215084.48 examples/s]\u001b[A\n",
      "Generating train split: 517000 examples [00:02, 213556.55 examples/s]\u001b[A\n",
      "Generating train split: 539000 examples [00:02, 214210.70 examples/s]\u001b[A\n",
      "Generating train split: 561000 examples [00:02, 215057.68 examples/s]\u001b[A\n",
      "Generating train split: 583000 examples [00:02, 215328.24 examples/s]\u001b[A\n",
      "Generating train split: 605000 examples [00:02, 214307.56 examples/s]\u001b[A\n",
      "Generating train split: 627000 examples [00:02, 214606.46 examples/s]\u001b[A\n",
      "Generating train split: 649000 examples [00:03, 214787.35 examples/s]\u001b[A\n",
      "Generating train split: 671000 examples [00:03, 214864.67 examples/s]\u001b[A\n",
      "Generating train split: 693000 examples [00:03, 214991.39 examples/s]\u001b[A\n",
      "                                                                     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generator downloaded and prepared to /home/prodadmin/.cache/huggingface/datasets/generator/default-539c4d3dc8c83bf1/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving the dataset (0/2 shards):   0%|                                                                                                                                                                                      | 0/629797 [00:00<?, ? examples/s]\u001b[A\n",
      "Saving the dataset (0/2 shards):   1%|██▏                                                                                                                                                                     | 8000/629797 [00:00<00:08, 69651.85 examples/s]\u001b[A\n",
      "Saving the dataset (0/2 shards):   3%|████▌                                                                                                                                                                  | 17000/629797 [00:00<00:08, 76157.98 examples/s]\u001b[A\n",
      "Saving the dataset (0/2 shards):   4%|██████▉                                                                                                                                                                | 26000/629797 [00:00<00:07, 78090.06 examples/s]\u001b[A\n",
      "Saving the dataset (0/2 shards):   6%|█████████▎                                                                                                                                                             | 35000/629797 [00:00<00:07, 79351.16 examples/s]\u001b[A\n",
      "Saving the dataset (0/2 shards):   7%|███████████▉                                                                                                                                                           | 45000/629797 [00:00<00:07, 80235.39 examples/s]\u001b[A\n",
      "Saving the dataset (0/2 shards):   9%|██████████████▌                                                                                                                                                        | 55000/629797 [00:00<00:07, 80684.11 examples/s]\u001b[A\n",
      "Saving the dataset (0/2 shards):  10%|█████████████████▏                                                                                                                                                     | 65000/629797 [00:00<00:06, 80784.10 examples/s]\u001b[A\n",
      "Saving the dataset (0/2 shards):  12%|███████████████████▉                                                                                                                                                   | 75000/629797 [00:00<00:06, 80936.13 examples/s]\u001b[A\n",
      "Saving the dataset (0/2 shards):  13%|██████████████████████▌                                                                                                                                                | 85000/629797 [00:01<00:06, 81149.71 examples/s]\u001b[A\n",
      "Saving the dataset (0/2 shards):  15%|█████████████████████████▏                                                                                                                                             | 95000/629797 [00:01<00:06, 81130.51 examples/s]\u001b[A\n",
      "Saving the dataset (0/2 shards):  17%|███████████████████████████▋                                                                                                                                          | 105000/629797 [00:01<00:06, 81135.12 examples/s]\u001b[A\n",
      "Saving the dataset (0/2 shards):  19%|███████████████████████████████                                                                                                                                       | 118000/629797 [00:01<00:06, 80572.72 examples/s]\u001b[A\n",
      "Saving the dataset (0/2 shards):  20%|█████████████████████████████████▋                                                                                                                                    | 128000/629797 [00:01<00:06, 80787.02 examples/s]\u001b[A\n",
      "Saving the dataset (0/2 shards):  22%|████████████████████████████████████▎                                                                                                                                 | 138000/629797 [00:01<00:06, 80884.50 examples/s]\u001b[A\n",
      "Saving the dataset (0/2 shards):  23%|██████████████████████████████████████▋                                                                                                                               | 147000/629797 [00:01<00:05, 80930.42 examples/s]\u001b[A\n",
      "Saving the dataset (0/2 shards):  25%|█████████████████████████████████████████                                                                                                                             | 156000/629797 [00:01<00:05, 80664.71 examples/s]\u001b[A\n",
      "Saving the dataset (0/2 shards):  26%|███████████████████████████████████████████▍                                                                                                                          | 165000/629797 [00:02<00:05, 80584.07 examples/s]\u001b[A\n",
      "Saving the dataset (0/2 shards):  28%|██████████████████████████████████████████████▉                                                                                                                       | 178000/629797 [00:02<00:05, 80238.89 examples/s]\u001b[A\n",
      "Saving the dataset (0/2 shards):  30%|█████████████████████████████████████████████████▎                                                                                                                    | 187000/629797 [00:02<00:05, 80216.96 examples/s]\u001b[A\n",
      "Saving the dataset (0/2 shards):  31%|███████████████████████████████████████████████████▋                                                                                                                  | 196000/629797 [00:02<00:05, 80085.14 examples/s]\u001b[A\n",
      "Saving the dataset (0/2 shards):  33%|██████████████████████████████████████████████████████                                                                                                                | 205000/629797 [00:02<00:05, 80015.23 examples/s]\u001b[A\n",
      "Saving the dataset (0/2 shards):  34%|████████████████████████████████████████████████████████▍                                                                                                             | 214000/629797 [00:02<00:05, 79981.04 examples/s]\u001b[A\n",
      "Saving the dataset (0/2 shards):  35%|██████████████████████████████████████████████████████████▊                                                                                                           | 223000/629797 [00:02<00:05, 79846.77 examples/s]\u001b[A\n",
      "Saving the dataset (0/2 shards):  37%|████████████████████████████████████████████████████████████▉                                                                                                         | 231000/629797 [00:02<00:05, 78065.88 examples/s]\u001b[A\n",
      "Saving the dataset (0/2 shards):  38%|███████████████████████████████████████████████████████████████▎                                                                                                      | 240000/629797 [00:03<00:08, 46768.52 examples/s]\u001b[A\n",
      "Saving the dataset (0/2 shards):  39%|█████████████████████████████████████████████████████████████████▎                                                                                                    | 248000/629797 [00:03<00:07, 51776.33 examples/s]\u001b[A\n",
      "Saving the dataset (0/2 shards):  41%|███████████████████████████████████████████████████████████████████▍                                                                                                  | 256000/629797 [00:03<00:06, 56179.45 examples/s]\u001b[A\n",
      "Saving the dataset (0/2 shards):  42%|█████████████████████████████████████████████████████████████████████▌                                                                                                | 264000/629797 [00:03<00:06, 59959.66 examples/s]\u001b[A\n",
      "Saving the dataset (0/2 shards):  43%|███████████████████████████████████████████████████████████████████████▉                                                                                              | 273000/629797 [00:03<00:05, 64709.81 examples/s]\u001b[A\n",
      "Saving the dataset (0/2 shards):  45%|██████████████████████████████████████████████████████████████████████████▌                                                                                           | 283000/629797 [00:03<00:04, 69591.56 examples/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (0/2 shards):  47%|█████████████████████████████████████████████████████████████████████████████▏                                                                                        | 293000/629797 [00:03<00:04, 73248.44 examples/s]\u001b[A\n",
      "Saving the dataset (0/2 shards):  48%|███████████████████████████████████████████████████████████████████████████████▊                                                                                      | 303000/629797 [00:04<00:04, 75627.04 examples/s]\u001b[A\n",
      "Saving the dataset (0/2 shards):  50%|██████████████████████████████████████████████████████████████████████████████████▍                                                                                   | 313000/629797 [00:04<00:04, 77324.42 examples/s]\u001b[A\n",
      "Saving the dataset (1/2 shards):  50%|███████████████████████████████████████████████████████████████████████████████████                                                                                   | 314899/629797 [00:04<00:04, 77324.42 examples/s]\u001b[A\n",
      "                                                                                                                                                                                                                                                              \u001b[A\n",
      "Saving the dataset (0/1 shards):   0%|                                                                                                                                                                                       | 0/69978 [00:00<?, ? examples/s]\u001b[A\n",
      "Saving the dataset (0/1 shards):  11%|███████████████████▎                                                                                                                                                     | 8000/69978 [00:00<00:03, 20641.81 examples/s]\u001b[A\n",
      "Saving the dataset (0/1 shards):  23%|██████████████████████████████████████▍                                                                                                                                 | 16000/69978 [00:00<00:01, 34950.14 examples/s]\u001b[A\n",
      "Saving the dataset (0/1 shards):  34%|█████████████████████████████████████████████████████████▌                                                                                                              | 24000/69978 [00:00<00:01, 45159.68 examples/s]\u001b[A\n",
      "Saving the dataset (0/1 shards):  46%|████████████████████████████████████████████████████████████████████████████▊                                                                                           | 32000/69978 [00:00<00:00, 52190.60 examples/s]\u001b[A\n",
      "Saving the dataset (0/1 shards):  57%|████████████████████████████████████████████████████████████████████████████████████████████████                                                                        | 40000/69978 [00:00<00:00, 57360.60 examples/s]\u001b[A\n",
      "Saving the dataset (0/1 shards):  69%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                    | 48000/69978 [00:00<00:00, 60698.55 examples/s]\u001b[A\n",
      "Saving the dataset (0/1 shards):  80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                 | 56000/69978 [00:01<00:00, 62943.87 examples/s]\u001b[A\n",
      "Saving the dataset (0/1 shards):  91%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋              | 64000/69978 [00:01<00:00, 64187.55 examples/s]\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 69978/69978 [00:01<00:00, 64187.55 examples/s]\u001b[A\n",
      "                                                                                                                                                                                                                                                              \u001b[A"
     ]
    }
   ],
   "source": [
    "import json\n",
    "data = []\n",
    "max_question_length=2000\n",
    "max_answer_length=2000\n",
    "with open(\"/opt/home/bo_ling/dataset/doc_transcript_pii_data.json\") as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "for d in json_data:\n",
    "    try:\n",
    "        instruction = d['instruction']+\":\"\n",
    "        input_text = d[\"input\"][:max_question_length]\n",
    "        output_text = \"\\n\" + d[\"output\"][:max_answer_length]\n",
    "        text = PROMPT_FORMAT.format(instruction=instruction,input_text=input_text,output_text=output_text)\n",
    "        data.append({\n",
    "                \"instruction\": instruction,\n",
    "                \"input\": input_text, \n",
    "                \"output\": output_text,\n",
    "                \"text\": text\n",
    "            })\n",
    "    except:\n",
    "        pass\n",
    "def gen():\n",
    "    for d in data:\n",
    "        yield d\n",
    "dataset = Dataset.from_generator(gen)\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "dataset.save_to_disk(\"/opt/home/bo_ling/dataset/doc_transcript_pii_data_simplify.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e884cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'text'],\n",
       "        num_rows: 629797\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'text'],\n",
       "        num_rows: 69978\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset = load_from_disk(\"/opt/home/bo_ling/dataset/doc_transcript_pii_data_simplify.hf\")\n",
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a550e946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Extract issue date from the following input:',\n",
       " 'input': 'Secretary Hal Taylor Secretary of Law Enforcement DRIVER LICENSE ALABAMA NO.8193988 CLASS D D.O.B. 01-24-1991 EXP 05-13-2024 JUSTIN DEWAYNE JONES 540 FRANCIS PL SW BIRMINGHAM AL 35211-2508 ENDORSEMENTS ISS 05-08-2020 Juste REST SEX M HT 5-05 EYES BRO WT 183 HAIR BLK',\n",
       " 'output': '\\n2020-05-08',\n",
       " 'text': 'Extract issue date from the following input:\\nSecretary Hal Taylor Secretary of Law Enforcement DRIVER LICENSE ALABAMA NO.8193988 CLASS D D.O.B. 01-24-1991 EXP 05-13-2024 JUSTIN DEWAYNE JONES 540 FRANCIS PL SW BIRMINGHAM AL 35211-2508 ENDORSEMENTS ISS 05-08-2020 Juste REST SEX M HT 5-05 EYES BRO WT 183 HAIR BLK\\n\\n### ### Response:\\n:\\n2020-05-08\\n'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a174c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset generator/default to /home/prodadmin/.cache/huggingface/datasets/generator/default-7895c5e842d084b9/0.0.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Generating train split: 0 examples [00:00, ? examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 1796 examples [00:00, 17886.03 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 4053 examples [00:00, 20631.90 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 6371 examples [00:00, 21793.42 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 8686 examples [00:00, 22327.86 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 11000 examples [00:00, 22468.39 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 13360 examples [00:00, 22846.81 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 15714 examples [00:00, 23069.83 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 19192 examples [00:00, 23077.28 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 21533 examples [00:00, 23166.52 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 23885 examples [00:01, 23264.08 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 27380 examples [00:01, 23230.48 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 29736 examples [00:01, 23316.73 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 33173 examples [00:01, 23167.25 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 35542 examples [00:01, 23298.82 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 37882 examples [00:01, 23322.73 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 41400 examples [00:01, 23169.30 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 43775 examples [00:01, 23314.00 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 47274 examples [00:02, 23315.32 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 49643 examples [00:02, 23408.09 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 52000 examples [00:02, 23305.93 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 54378 examples [00:02, 23432.59 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 56736 examples [00:02, 23471.60 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 60223 examples [00:02, 23384.28 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 62580 examples [00:02, 23431.83 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 64971 examples [00:02, 23559.78 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 68418 examples [00:02, 23341.39 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 70796 examples [00:03, 23453.60 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 74249 examples [00:03, 23295.69 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 76604 examples [00:03, 23357.44 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 78976 examples [00:03, 23451.69 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 82485 examples [00:03, 23426.45 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 85944 examples [00:03, 23298.92 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 89410 examples [00:03, 23233.97 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 91770 examples [00:03, 23320.59 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 95240 examples [00:04, 23253.33 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 97599 examples [00:04, 23332.98 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 99963 examples [00:04, 23411.33 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 103419 examples [00:04, 23277.09 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 105772 examples [00:04, 23340.77 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 109126 examples [00:04, 22945.00 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 111484 examples [00:04, 23102.58 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 113848 examples [00:04, 23241.90 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 117363 examples [00:05, 23153.40 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 119703 examples [00:05, 23215.88 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 123196 examples [00:05, 23204.27 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 125565 examples [00:05, 23323.81 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 127921 examples [00:05, 23383.84 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 131398 examples [00:05, 23211.10 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 133759 examples [00:05, 23308.83 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 137273 examples [00:05, 23348.45 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 139655 examples [00:06, 23464.80 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 143197 examples [00:06, 23321.86 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 145564 examples [00:06, 23406.01 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 147922 examples [00:06, 23448.37 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 151399 examples [00:06, 23260.50 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 153761 examples [00:06, 23351.04 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 157197 examples [00:06, 23189.18 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 159538 examples [00:06, 23242.26 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 163000 examples [00:07, 23095.79 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 165351 examples [00:07, 23196.46 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 168810 examples [00:07, 23145.83 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "                                                                    \u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generator downloaded and prepared to /home/prodadmin/.cache/huggingface/datasets/generator/default-7895c5e842d084b9/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Saving the dataset (0/1 shards):   0%|                                                                                                                                                                                      | 0/152615 [00:00<?, ? examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Saving the dataset (0/1 shards):  12%|███████████████████▌                                                                                                                                                  | 18000/152615 [00:00<00:00, 173411.10 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Saving the dataset (0/1 shards):  26%|███████████████████████████████████████████▌                                                                                                                          | 40000/152615 [00:00<00:00, 195499.43 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Saving the dataset (0/1 shards):  41%|███████████████████████████████████████████████████████████████████▍                                                                                                  | 62000/152615 [00:00<00:00, 204141.24 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Saving the dataset (0/1 shards):  55%|███████████████████████████████████████████████████████████████████████████████████████████▎                                                                          | 84000/152615 [00:00<00:00, 208141.68 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Saving the dataset (0/1 shards):  69%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                  | 106000/152615 [00:00<00:00, 210292.04 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Saving the dataset (0/1 shards):  84%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                          | 128000/152615 [00:00<00:00, 212110.46 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Saving the dataset (0/1 shards):  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  | 150000/152615 [00:00<00:00, 212473.75 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Saving the dataset (1/1 shards): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 152615/152615 [00:00<00:00, 212473.75 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                                                                                                                                                                                              \u001b[A\u001b[A\n",
      "\n",
      "Saving the dataset (0/1 shards):   0%|                                                                                                                                                                                       | 0/16958 [00:00<?, ? examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Saving the dataset (1/1 shards): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16958/16958 [00:00<00:00, 209322.06 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                                                                                                                                                                                              \u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "parquet_file = \"/opt/home/bo_ling/dataset/modeling_data_v1.parquet\"\n",
    "import pandas as pd\n",
    "df = pd.read_parquet(parquet_file, engine='pyarrow')\n",
    "count = 0\n",
    "\n",
    "def gen():\n",
    "    for index, row in df.iterrows():\n",
    "        yield {\"text\": row['context_and_msg']}\n",
    "dataset = Dataset.from_generator(gen)\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "    \n",
    "dataset.save_to_disk(\"/opt/home/bo_ling/dataset/modeling_data_v1.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ede512e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 152615\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 16958\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset = load_from_disk(\"/opt/home/bo_ling/dataset/modeling_data_v1.hf\")\n",
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5dcd18c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Specific Information: The restaurant name is Insomnia Cookies (164 Orchard St) and the order value is $19.93. The estimated delivery time is 01:45 and latest arrival time is 02:05. Order status is 6.\\n\\n###\\n\\nagent: Hi Charlotte, welcome to the chat support team. I'll be assisting you today. <|endofsentence|>\\nagent: May I know your concern please? <|endofsentence|>\\nagent: Charlotte, Are we still connected? <|endofsentence|>\\nagent: I'm grateful that I was able to help you out today, Charlotte. As I have not heard back from you in a while, I am closing the chat. Please feel free to contact us again for any issues. Thank you for chatting with us today. Stay safe, stay healthy. <|endofsentence|>\\nagent: Yes No <|endofsentence|>\\nagent: Have we resolved this issue? <|endofsentence|>\\n\"}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c0bcbe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Specific Information: The restaurant name is Carlos & Gabbys / Graze Smokehouse / Mexiko (NYC) / Rosa's Coal Fire Chicken  and the order value is $24.07. The estimated delivery time is 19:30 and latest arrival time is 20:00. Order status is 6.\\n\\n###\\n\\nAmin: Hello <|endofsentence|>\\nAmin: I have a request for my order <|endofsentence|>\\nagent: Hi, Amin. Welcome to Chat Support. I'm Archana, and I'm here to help. <|endofsentence|>\\nAmin: Can u contact merchant <|endofsentence|>\\nagent: I understand your concern for the issue which you have faced. I am sorry for the inconvenience. <|endofsentence|>\\nAmin: Can u contact merchant for me with a request <|endofsentence|>\\nagent: I can understand your situation, Amin. I know this is inconvenient, but we do not currently provide phone support for these issues right now which is why we are unable to contact the restaurant from our end. <|endofsentence|>\\nagent: Have we resolved this issue? <|endofsentence|>\\nagent: Yes No <|endofsentence|>\\n\"}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cee6a904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset generator/default to /home/prodadmin/.cache/huggingface/datasets/generator/default-0e5ad272234cb383/0.0.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating train split: 0 examples [00:00, ? examples/s]\u001b[A\n",
      "Generating train split: 21078 examples [00:00, 210032.07 examples/s]\u001b[A\n",
      "Generating train split: 43000 examples [00:00, 212630.50 examples/s]\u001b[A\n",
      "Generating train split: 65018 examples [00:00, 214553.18 examples/s]\u001b[A\n",
      "Generating train split: 87229 examples [00:00, 216758.04 examples/s]\u001b[A\n",
      "Generating train split: 109207 examples [00:00, 217381.03 examples/s]\u001b[A\n",
      "Generating train split: 131000 examples [00:00, 216789.70 examples/s]\u001b[A\n",
      "Generating train split: 153469 examples [00:00, 218790.61 examples/s]\u001b[A\n",
      "Generating train split: 182662 examples [00:00, 219306.10 examples/s]\u001b[A\n",
      "                                                                     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generator downloaded and prepared to /home/prodadmin/.cache/huggingface/datasets/generator/default-0e5ad272234cb383/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving the dataset (0/1 shards):   0%|                                                                                                                                                                                      | 0/182662 [00:00<?, ? examples/s]\u001b[A\n",
      "Saving the dataset (0/1 shards):  68%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                   | 125000/182662 [00:00<00:00, 1219107.15 examples/s]\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 182662/182662 [00:00<00:00, 1219107.15 examples/s]\u001b[A\n",
      "                                                                                                                                                                                                                                                              \u001b[A"
     ]
    }
   ],
   "source": [
    "import json \n",
    "data = []\n",
    "max_question_length=2000\n",
    "max_answer_length=2000\n",
    "with open(\"/opt/home/bo_ling/dataset/gds_v4.json\") as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "for d in json_data:\n",
    "    try:\n",
    "        instruction = d['instruction'].strip().replace(\":\", \" input:\")\n",
    "        input_text = d[\"ocr_text\"][:max_question_length]\n",
    "        output_text = \"\\n\" + d[\"output\"][:max_answer_length]\n",
    "        text = PROMPT_FORMAT.format(instruction=instruction,input_text=input_text,output_text=output_text)\n",
    "        data.append({\n",
    "                \"instruction\": instruction,\n",
    "                \"input\": input_text, \n",
    "                \"output\": output_text,\n",
    "                \"text\": text\n",
    "            })\n",
    "    except:\n",
    "        pass\n",
    "def gen():\n",
    "    for d in data:\n",
    "        yield d\n",
    "dataset = Dataset.from_generator(gen)\n",
    "dataset.save_to_disk(\"/opt/home/bo_ling/dataset/gds_v4_simplify.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f4bdbd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182662"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dfa2bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'text'],\n",
       "    num_rows: 182662\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = load_from_disk(\"/opt/home/bo_ling/dataset/gds_v4_simplify.hf\")\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf5b1651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Extract dob from the following input:',\n",
       " 'input': 'California DRIVER LICENSE 081202 LN SOTO MUNGUIA FN ALEC PIERRE 9939 VAN RUITEN ST BELLFLOWER, CA 90706 SEX M AM DONOR USA DLY7679862 EXP 08/12/2025 DOB 08/12/2002 AGE 2 IN 7023 DD 08/04/2021606A3/E5FD/25 CLASS C END NONE RSTR NONE 08122002 HAIR BRN EYES HZL HGT 5\\'-11\" WGT 205 lb FEDERAL LIMITS APPLY ISS 10/14/2021',\n",
       " 'output': '\\n2002-08-12',\n",
       " 'text': 'Extract dob from the following input:\\nCalifornia DRIVER LICENSE 081202 LN SOTO MUNGUIA FN ALEC PIERRE 9939 VAN RUITEN ST BELLFLOWER, CA 90706 SEX M AM DONOR USA DLY7679862 EXP 08/12/2025 DOB 08/12/2002 AGE 2 IN 7023 DD 08/04/2021606A3/E5FD/25 CLASS C END NONE RSTR NONE 08122002 HAIR BRN EYES HZL HGT 5\\'-11\" WGT 205 lb FEDERAL LIMITS APPLY ISS 10/14/2021\\n\\n### ### Response:\\n:\\n2002-08-12\\n'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9b1fa4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Extract dob from the following input:': 75,\n",
       " 'Extract issue date from the following input:': 75,\n",
       " 'Extract expiration date from the following input:': 75,\n",
       " 'Extract first name from the following input:': 74,\n",
       " 'Extract middle name from the following input:': 64,\n",
       " 'Extract last name from the following input:': 74,\n",
       " 'Extract license class from the following input:': 74,\n",
       " 'Extract drivers license number from the following input:': 74,\n",
       " 'Extract zip from the following input:': 74,\n",
       " 'Extract driving licence issue state from the following input:': 74,\n",
       " 'Extract address from the following input:': 74,\n",
       " 'Extract gender from the following input:': 74,\n",
       " 'Extract city from the following input:': 74,\n",
       " 'Is the driving license valid for identification?': 44}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "ins_count = {}\n",
    "for d in dataset:\n",
    "    count += 1\n",
    "    if count >=1000:\n",
    "        break\n",
    "    instruction = d['instruction']\n",
    "    ins_count[instruction] = ins_count.get(instruction, 0) + 1\n",
    "ins_count   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e3ec59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Extract address from the following input:',\n",
       " 'input': 'California DRIVER LICENSE 081202 LN SOTO MUNGUIA FN ALEC PIERRE 9939 VAN RUITEN ST BELLFLOWER, CA 90706 SEX M AM DONOR USA DLY7679862 EXP 08/12/2025 DOB 08/12/2002 AGE 2 IN 7023 DD 08/04/2021606A3/E5FD/25 CLASS C END NONE RSTR NONE 08122002 HAIR BRN EYES HZL HGT 5\\'-11\" WGT 205 lb FEDERAL LIMITS APPLY ISS 10/14/2021',\n",
       " 'output': '\\n9939 VAN RUITEN ST',\n",
       " 'text': 'Extract address from the following input:\\nCalifornia DRIVER LICENSE 081202 LN SOTO MUNGUIA FN ALEC PIERRE 9939 VAN RUITEN ST BELLFLOWER, CA 90706 SEX M AM DONOR USA DLY7679862 EXP 08/12/2025 DOB 08/12/2002 AGE 2 IN 7023 DD 08/04/2021606A3/E5FD/25 CLASS C END NONE RSTR NONE 08122002 HAIR BRN EYES HZL HGT 5\\'-11\" WGT 205 lb FEDERAL LIMITS APPLY ISS 10/14/2021\\n\\n### ### Response:\\n:\\n9939 VAN RUITEN ST\\n'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "570e9906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517 172\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    ")\n",
    "local_output_dir=\"/opt/home/doc_transcript_pii_checkpoint-49200\"\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_output_dir, padding_side=\"left\")\n",
    "print(len(dataset[10]['text']), len(tokenizer.encode(dataset[10]['text'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba43f38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

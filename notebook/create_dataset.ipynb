{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9eb577b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "SCRIPT_DIR = os.path.dirname(os.path.abspath(\"...\"))\n",
    "sys.path.append(os.path.dirname(SCRIPT_DIR))\n",
    "from training.generate import (generate_response, load_model_tokenizer_for_generate, \n",
    "                               get_special_token_id, get_special_token_id)\n",
    "from training.transcript_trainer import PROMPT_FORMAT, create_data_set_from_json_list\n",
    "from datasets import Dataset, DatasetDict, load_dataset, load_from_disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4366efd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset generator/default to /home/prodadmin/.cache/huggingface/datasets/generator/default-d3892b7dde97b332/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba6528cea674cb4a70f2d1a566dcb22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generator downloaded and prepared to /home/prodadmin/.cache/huggingface/datasets/generator/default-d3892b7dde97b332/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "466514a4ff674a449b07b324462a3c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/629797 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/69978 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "data = []\n",
    "max_question_length=2000\n",
    "max_answer_length=2000\n",
    "with open(\"/opt/home/bo_ling/dataset/doc_transcript_pii_data.json\") as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "for d in json_data:\n",
    "    try:\n",
    "        instruction = d['instruction']+\":\"\n",
    "        input_text = d[\"input\"][:max_question_length]\n",
    "        output_text = \"\\n\" + d[\"output\"][:max_answer_length]\n",
    "        text = PROMPT_FORMAT.format(instruction=instruction,input_text=input_text,output_text=output_text)\n",
    "        data.append({\n",
    "                \"instruction\": instruction,\n",
    "                \"input\": input_text, \n",
    "                \"output\": output_text,\n",
    "                \"text\": text\n",
    "            })\n",
    "    except:\n",
    "        pass\n",
    "def gen():\n",
    "    for d in data:\n",
    "        yield d\n",
    "dataset = Dataset.from_generator(gen)\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "dataset.save_to_disk(\"/opt/home/bo_ling/dataset/doc_transcript_pii_data_corrected.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e884cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'text'],\n",
       "        num_rows: 629797\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'text'],\n",
       "        num_rows: 69978\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset = load_from_disk(\"/opt/home/bo_ling/dataset/doc_transcript_pii_data_corrected.hf\")\n",
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a550e946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Extract gender from the following input:',\n",
       " 'input': 'MASSACHUSETTS Games Tesler REGISTRAR MAS Sachbes NOT FOR FEDERAL ID MAS 4a ISS 46 EXP DRIVER\\'S NWEALT LICENSE HUSETY 06/04/2021 06/23/2026 9 CLASS 12 REST D NONE 18 EYES BRO 15 SEX F 16 HGT 5\\'-06\" DONOR MAMA TORRES 2 MARIA YSABEL 8 21 MAPLE ST ATTLEBORO, MA 02703-4013 5 DD 06/07/2021 Rev 02/22/2016 MASSACHUSETTS 4d NUMBER S05276571 06/23/1976 9a END NONE 3 DOB 06/23/76 MASS ASMA MASSACHUSETTS MASSACHUSE N hmotn',\n",
       " 'output': '\\nmale',\n",
       " 'text': 'Extract gender from the following input:\\nMASSACHUSETTS Games Tesler REGISTRAR MAS Sachbes NOT FOR FEDERAL ID MAS 4a ISS 46 EXP DRIVER\\'S NWEALT LICENSE HUSETY 06/04/2021 06/23/2026 9 CLASS 12 REST D NONE 18 EYES BRO 15 SEX F 16 HGT 5\\'-06\" DONOR MAMA TORRES 2 MARIA YSABEL 8 21 MAPLE ST ATTLEBORO, MA 02703-4013 5 DD 06/07/2021 Rev 02/22/2016 MASSACHUSETTS 4d NUMBER S05276571 06/23/1976 9a END NONE 3 DOB 06/23/76 MASS ASMA MASSACHUSETTS MASSACHUSE N hmotn\\n\\n### Response:\\n\\nmale\\n'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset['test'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a174c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset generator/default to /home/prodadmin/.cache/huggingface/datasets/generator/default-7895c5e842d084b9/0.0.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Generating train split: 0 examples [00:00, ? examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 1796 examples [00:00, 17886.03 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 4053 examples [00:00, 20631.90 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 6371 examples [00:00, 21793.42 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 8686 examples [00:00, 22327.86 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 11000 examples [00:00, 22468.39 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 13360 examples [00:00, 22846.81 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 15714 examples [00:00, 23069.83 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 19192 examples [00:00, 23077.28 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 21533 examples [00:00, 23166.52 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 23885 examples [00:01, 23264.08 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 27380 examples [00:01, 23230.48 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 29736 examples [00:01, 23316.73 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 33173 examples [00:01, 23167.25 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 35542 examples [00:01, 23298.82 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 37882 examples [00:01, 23322.73 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 41400 examples [00:01, 23169.30 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 43775 examples [00:01, 23314.00 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 47274 examples [00:02, 23315.32 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 49643 examples [00:02, 23408.09 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 52000 examples [00:02, 23305.93 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 54378 examples [00:02, 23432.59 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 56736 examples [00:02, 23471.60 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 60223 examples [00:02, 23384.28 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 62580 examples [00:02, 23431.83 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 64971 examples [00:02, 23559.78 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 68418 examples [00:02, 23341.39 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 70796 examples [00:03, 23453.60 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 74249 examples [00:03, 23295.69 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 76604 examples [00:03, 23357.44 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 78976 examples [00:03, 23451.69 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 82485 examples [00:03, 23426.45 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 85944 examples [00:03, 23298.92 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 89410 examples [00:03, 23233.97 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 91770 examples [00:03, 23320.59 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 95240 examples [00:04, 23253.33 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 97599 examples [00:04, 23332.98 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 99963 examples [00:04, 23411.33 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 103419 examples [00:04, 23277.09 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 105772 examples [00:04, 23340.77 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 109126 examples [00:04, 22945.00 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 111484 examples [00:04, 23102.58 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 113848 examples [00:04, 23241.90 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 117363 examples [00:05, 23153.40 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 119703 examples [00:05, 23215.88 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 123196 examples [00:05, 23204.27 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 125565 examples [00:05, 23323.81 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 127921 examples [00:05, 23383.84 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 131398 examples [00:05, 23211.10 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 133759 examples [00:05, 23308.83 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 137273 examples [00:05, 23348.45 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 139655 examples [00:06, 23464.80 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 143197 examples [00:06, 23321.86 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 145564 examples [00:06, 23406.01 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 147922 examples [00:06, 23448.37 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 151399 examples [00:06, 23260.50 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 153761 examples [00:06, 23351.04 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 157197 examples [00:06, 23189.18 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 159538 examples [00:06, 23242.26 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 163000 examples [00:07, 23095.79 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 165351 examples [00:07, 23196.46 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating train split: 168810 examples [00:07, 23145.83 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "                                                                    \u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generator downloaded and prepared to /home/prodadmin/.cache/huggingface/datasets/generator/default-7895c5e842d084b9/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Saving the dataset (0/1 shards):   0%|                                                                                                                                                                                      | 0/152615 [00:00<?, ? examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Saving the dataset (0/1 shards):  12%|███████████████████▌                                                                                                                                                  | 18000/152615 [00:00<00:00, 173411.10 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Saving the dataset (0/1 shards):  26%|███████████████████████████████████████████▌                                                                                                                          | 40000/152615 [00:00<00:00, 195499.43 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Saving the dataset (0/1 shards):  41%|███████████████████████████████████████████████████████████████████▍                                                                                                  | 62000/152615 [00:00<00:00, 204141.24 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Saving the dataset (0/1 shards):  55%|███████████████████████████████████████████████████████████████████████████████████████████▎                                                                          | 84000/152615 [00:00<00:00, 208141.68 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Saving the dataset (0/1 shards):  69%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                  | 106000/152615 [00:00<00:00, 210292.04 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Saving the dataset (0/1 shards):  84%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                          | 128000/152615 [00:00<00:00, 212110.46 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Saving the dataset (0/1 shards):  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  | 150000/152615 [00:00<00:00, 212473.75 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Saving the dataset (1/1 shards): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 152615/152615 [00:00<00:00, 212473.75 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                                                                                                                                                                                              \u001b[A\u001b[A\n",
      "\n",
      "Saving the dataset (0/1 shards):   0%|                                                                                                                                                                                       | 0/16958 [00:00<?, ? examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Saving the dataset (1/1 shards): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16958/16958 [00:00<00:00, 209322.06 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                                                                                                                                                                                              \u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "parquet_file = \"/opt/home/bo_ling/dataset/modeling_data_v1.parquet\"\n",
    "import pandas as pd\n",
    "df = pd.read_parquet(parquet_file, engine='pyarrow')\n",
    "count = 0\n",
    "\n",
    "def gen():\n",
    "    for index, row in df.iterrows():\n",
    "        yield {\"text\": row['context_and_msg']}\n",
    "dataset = Dataset.from_generator(gen)\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "    \n",
    "dataset.save_to_disk(\"/opt/home/bo_ling/dataset/modeling_data_v1.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ede512e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 152615\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 16958\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset = load_from_disk(\"/opt/home/bo_ling/dataset/modeling_data_v1.hf\")\n",
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cee6a904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset generator/default to /home/prodadmin/.cache/huggingface/datasets/generator/default-82dc7c038733c089/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "363bbba1db0e4c7da212d5f5221a498d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generator downloaded and prepared to /home/prodadmin/.cache/huggingface/datasets/generator/default-82dc7c038733c089/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/182662 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json \n",
    "data = []\n",
    "max_question_length=2000\n",
    "max_answer_length=2000\n",
    "with open(\"/opt/home/bo_ling/dataset/gds_v4.json\") as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "for d in json_data:\n",
    "    try:\n",
    "        instruction = d['instruction'].strip().replace(\":\", \" input:\")\n",
    "        input_text = d[\"ocr_text\"][:max_question_length]\n",
    "        output_text = \"\\n\" + d[\"output\"][:max_answer_length]\n",
    "        text = PROMPT_FORMAT.format(instruction=instruction,input_text=input_text,output_text=output_text)\n",
    "        data.append({\n",
    "                \"instruction\": instruction,\n",
    "                \"input\": input_text, \n",
    "                \"output\": output_text,\n",
    "                \"text\": text\n",
    "            })\n",
    "    except:\n",
    "        pass\n",
    "def gen():\n",
    "    for d in data:\n",
    "        yield d\n",
    "dataset = Dataset.from_generator(gen)\n",
    "dataset.save_to_disk(\"/opt/home/bo_ling/dataset/gds_v4_simplify.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dfa2bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'text'],\n",
       "    num_rows: 182662\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = load_from_disk(\"/opt/home/bo_ling/dataset/gds_v4_simplify.hf\")\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fb74492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Extract dob from the following input:',\n",
       " 'input': 'California DRIVER LICENSE 081202 LN SOTO MUNGUIA FN ALEC PIERRE 9939 VAN RUITEN ST BELLFLOWER, CA 90706 SEX M AM DONOR USA DLY7679862 EXP 08/12/2025 DOB 08/12/2002 AGE 2 IN 7023 DD 08/04/2021606A3/E5FD/25 CLASS C END NONE RSTR NONE 08122002 HAIR BRN EYES HZL HGT 5\\'-11\" WGT 205 lb FEDERAL LIMITS APPLY ISS 10/14/2021',\n",
       " 'output': '\\n2002-08-12',\n",
       " 'text': 'Extract dob from the following input:\\nCalifornia DRIVER LICENSE 081202 LN SOTO MUNGUIA FN ALEC PIERRE 9939 VAN RUITEN ST BELLFLOWER, CA 90706 SEX M AM DONOR USA DLY7679862 EXP 08/12/2025 DOB 08/12/2002 AGE 2 IN 7023 DD 08/04/2021606A3/E5FD/25 CLASS C END NONE RSTR NONE 08122002 HAIR BRN EYES HZL HGT 5\\'-11\" WGT 205 lb FEDERAL LIMITS APPLY ISS 10/14/2021\\n\\n### Response:\\n\\n2002-08-12\\n'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9b1fa4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Extract dob from the following input:': 75,\n",
       " 'Extract issue date from the following input:': 75,\n",
       " 'Extract expiration date from the following input:': 75,\n",
       " 'Extract first name from the following input:': 74,\n",
       " 'Extract middle name from the following input:': 64,\n",
       " 'Extract last name from the following input:': 74,\n",
       " 'Extract license class from the following input:': 74,\n",
       " 'Extract drivers license number from the following input:': 74,\n",
       " 'Extract zip from the following input:': 74,\n",
       " 'Extract driving licence issue state from the following input:': 74,\n",
       " 'Extract address from the following input:': 74,\n",
       " 'Extract gender from the following input:': 74,\n",
       " 'Extract city from the following input:': 74,\n",
       " 'Is the driving license valid for identification?': 44}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "ins_count = {}\n",
    "for d in dataset:\n",
    "    count += 1\n",
    "    if count >=1000:\n",
    "        break\n",
    "    instruction = d['instruction']\n",
    "    ins_count[instruction] = ins_count.get(instruction, 0) + 1\n",
    "ins_count   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e13758dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset generator/default to /home/prodadmin/.cache/huggingface/datasets/generator/default-fd99f3d44c022861/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f70ab32e77b4396848aa21c1410ca27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generator downloaded and prepared to /home/prodadmin/.cache/huggingface/datasets/generator/default-fd99f3d44c022861/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/629797 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/69978 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "data = []\n",
    "max_question_length=2000\n",
    "max_answer_length=2000\n",
    "with open(\"/opt/home/bo_ling/dataset/doc_transcript_pii_data.json\") as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "for d in json_data:\n",
    "    try:\n",
    "        instruction = d['instruction']+\": \"\n",
    "        input_text = d[\"input\"][:max_question_length]\n",
    "        output_text = d[\"output\"][:max_answer_length]\n",
    "        text = PROMPT_FORMAT.format(instruction=instruction,input_text=input_text,output_text=output_text)\n",
    "        data.append({\n",
    "                \"input_text\": instruction + input_text,\n",
    "                \"output_text\": output_text,\n",
    "            })\n",
    "    except:\n",
    "        pass\n",
    "def gen():\n",
    "    for d in data:\n",
    "        yield d\n",
    "dataset = Dataset.from_generator(gen)\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "dataset.save_to_disk(\"/opt/home/bo_ling/dataset/doc_transcript_pii_data_t5.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "691546a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_text', 'output_text'],\n",
       "        num_rows: 629797\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_text', 'output_text'],\n",
       "        num_rows: 69978\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = load_from_disk(\"/opt/home/bo_ling/dataset/doc_transcript_pii_data_t5.hf\")\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84a1309d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_text': 'Extract zip from the following input: Connecticut Jebongili Мадивана COMMISSIONER Mafile Jan DRIVER LICENSE DL 40 LIC #028369950 3 DOB 02/09/1979 40 EXP 02/09/2029 9 CLASS D 9a END NONE 12 REST B 4a ISS 04/21/2021 15SEX M 16 HGT 5\\'-09\" 18 EYES GRN 5 DD 21042114520701MV8W USA 1 TURAN 2 MUSTAFA 8 89 COLEMAN ST 713 WEST HAVEN, CT 06516-7413',\n",
       " 'output_text': '06516'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da351643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset generator (/home/prodadmin/.cache/huggingface/datasets/generator/default-cb13d9f7f4f0a367/0.0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'uuid': 'c65024cc-26bc-490a-91d2-62f840fc0f8b',\n",
       " 'field': 'dob',\n",
       " 'input_text': 'Extract dob from the following input: California DRIVER LICENSE 081202 LN SOTO MUNGUIA FN ALEC PIERRE 9939 VAN RUITEN ST BELLFLOWER, CA 90706 SEX M AM DONOR USA DLY7679862 EXP 08/12/2025 DOB 08/12/2002 AGE 2 IN 7023 DD 08/04/2021606A3/E5FD/25 CLASS C END NONE RSTR NONE 08122002 HAIR BRN EYES HZL HGT 5\\'-11\" WGT 205 lb FEDERAL LIMITS APPLY ISS 10/14/2021',\n",
       " 'output_text': '2002-08-12'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "data = []\n",
    "max_question_length=2000\n",
    "max_answer_length=2000\n",
    "with open(\"/opt/home/bo_ling/dataset/gds_v4.json\") as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "for d in json_data:\n",
    "    try:\n",
    "        instruction = d['instruction'].strip().replace(\":\", \" input: \")\n",
    "        input_text = d[\"ocr_text\"][:max_question_length]\n",
    "        output_text = d[\"output\"][:max_answer_length]\n",
    "        text = PROMPT_FORMAT.format(instruction=instruction,input_text=input_text,output_text=output_text)\n",
    "        data.append({\n",
    "                \"uuid\": d[\"uuid\"],\n",
    "                \"field\": d[\"field\"],\n",
    "                \"input_text\": instruction + input_text,\n",
    "                \"output_text\": output_text,\n",
    "            })\n",
    "    except:\n",
    "        pass\n",
    "def gen():\n",
    "    for d in data:\n",
    "        yield d\n",
    "dataset = Dataset.from_generator(gen)\n",
    "dataset[0]\n",
    "#dataset.save_to_disk(\"/opt/home/bo_ling/dataset/gds_v4_t5.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "168430f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/182662 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.save_to_disk(\"/opt/home/bo_ling/dataset/gds_v4_t5_uuid.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38712d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset generator/default to /home/prodadmin/.cache/huggingface/datasets/generator/default-fd12e496e3ce2104/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb244f7502b94973a875410db56a33b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generator downloaded and prepared to /home/prodadmin/.cache/huggingface/datasets/generator/default-fd12e496e3ce2104/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/419 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "json_list_file = \"/opt/home/bo_ling/dataset/eats_receipt_gcp_test.jsonl\"\n",
    "def gen():\n",
    "    with open(json_list_file) as file:\n",
    "        while True:\n",
    "            line = file.readline()\n",
    "\n",
    "            # if line is empty\n",
    "            # end of file is reached\n",
    "            if not line:\n",
    "                break \n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "\n",
    "            except:\n",
    "                # print(f\"BAD DATA: {line}\")\n",
    "                data = json.loads(line.replace(\"\\\\\", \"\"))\n",
    "\n",
    "            yield{\n",
    "                \"input_text\": data['input_text'], \n",
    "                \"output_text\": data['output_text'].replace(\" total_price\", \"; total_price\").replace(\" total_tax\", \"; total_tax\")\n",
    "            }\n",
    "dataset = Dataset.from_generator(gen)\n",
    "dataset = dataset.train_test_split(test_size=0.99)\n",
    "dataset.save_to_disk(\"/opt/home/bo_ling/dataset/eats_receipt_gcp_test.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3fdb0c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_text', 'output_text'],\n",
       "        num_rows: 1693\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_text', 'output_text'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = load_from_disk(\"/opt/home/bo_ling/dataset/eats_receipt_gcp_train.hf\")\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16803f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_text', 'output_text'],\n",
       "        num_rows: 1906\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_text', 'output_text'],\n",
       "        num_rows: 212\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917bfad6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

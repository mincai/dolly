{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cc0ddec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "SCRIPT_DIR = os.path.dirname(os.path.abspath(\"...\"))\n",
    "sys.path.append(os.path.dirname(SCRIPT_DIR))\n",
    "\n",
    "from training.generate import (generate_response, load_model_tokenizer_for_generate, \n",
    "                               get_special_token_id, get_special_token_id)\n",
    "from training.consts import END_KEY, PROMPT_FORMAT, RESPONSE_KEY_NL\n",
    "from training.trainer import PROMPT_FORMAT, create_data_set_from_json_list\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bdb9ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'text'],\n",
       "    num_rows: 44\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_data = load_from_disk(\"/opt/home/bo_ling/dataset/michelangelo_so_long.hf\")[\"test\"]\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1b0ebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_output_dir=\"/opt/home/bo_ling/dolly_training/ma_helpdesk/checkpoint-400\"\n",
    "model, tokenizer = load_model_tokenizer_for_generate(local_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61f88a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_helpdesk_response(\n",
    "    instruction: str,\n",
    "    input_text: str,\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    do_sample: bool = True,\n",
    "    max_new_tokens: int = 256,\n",
    "    top_p: float = 0.92,\n",
    "    top_k: int = 0,\n",
    "    **kwargs,\n",
    ") -> str:\n",
    "    texts = PROMPT_FORMAT.format(instruction=instruction, input_text=input_text, output_text=\"\")\n",
    "    input_ids = tokenizer(texts, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "    response_key_token_id = get_special_token_id(tokenizer, RESPONSE_KEY_NL)\n",
    "    end_key_token_id = get_special_token_id(tokenizer, END_KEY)\n",
    "\n",
    "    gen_tokens = model.generate(\n",
    "        input_ids,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        # Ensure generation stops once it generates \"### End\"\n",
    "        eos_token_id=end_key_token_id,\n",
    "        do_sample=do_sample,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        **kwargs,\n",
    "    )[0].cpu()\n",
    "\n",
    "    # The response will be set to this variable if we can identify it.\n",
    "    decoded = None\n",
    "\n",
    "    # Find where \"### Response:\" is first found in the generated tokens.  Considering this is part of the prompt,\n",
    "    # we should definitely find it.  We will return the tokens found after this token.\n",
    "    response_pos = None\n",
    "    response_positions = np.where(gen_tokens == response_key_token_id)[0]\n",
    "    if len(response_positions) == 0:\n",
    "        logger.warn(f\"Could not find response key {response_key_token_id} in: {gen_tokens}\")\n",
    "    else:\n",
    "        response_pos = response_positions[0]\n",
    "\n",
    "    if response_pos:\n",
    "        # Next find where \"### End\" is located.  The model has been trained to end its responses with this sequence\n",
    "        # (or actually, the token ID it maps to, since it is a special token).  We may not find this token, as the\n",
    "        # response could be truncated.  If we don't find it then just return everything to the end.  Note that\n",
    "        # even though we set eos_token_id, we still see the this token at the end.\n",
    "        end_pos = None\n",
    "        end_positions = np.where(gen_tokens == end_key_token_id)[0]\n",
    "        if len(end_positions) > 0:\n",
    "            end_pos = end_positions[0]\n",
    "\n",
    "        decoded = tokenizer.decode(gen_tokens[response_pos + 1 : end_pos]).strip()\n",
    "\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e1c846c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "==================================================\n",
      "{'instruction': 'Please answer the following MA helpdesk questions:', 'input': 'Adding published fields for Michelangelo requires a retrain', 'output': '\\nI see you are adding new palette features into published fields that are not in the DSL, which cause the model validation to fail.For this kind of case, you need to update the template and train the model again.You can see the exception stack trace documented in this tickethttps:  t3.uberinternal.com browse MA-4578', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nPlease answer the following MA helpdesk questions:\\nAdding published fields for Michelangelo requires a retrain\\n\\n### Response:\\nI see you are adding new palette features into published fields that are not in the DSL, which cause the model validation to fail.For this kind of case, you need to update the template and train the model again.You can see the exception stack trace documented in this tickethttps:  t3.uberinternal.com browse MA-4578\\n'}\n",
      "**************************************************\n",
      "QUESTIONS:\n",
      "Adding published fields for Michelangelo requires a retrain\n",
      "\n",
      "GENERATED:\n",
      "Turns out I needed to just install cmake on my machine using brew install cmake\n",
      "\n",
      "EXPECTED:\n",
      "\n",
      "I see you are adding new palette features into published fields that are not in the DSL, which cause the model validation to fail.For this kind of case, you need to update the template and train the model again.You can see the exception stack trace documented in this tickethttps:  t3.uberinternal.com browse MA-4578\n",
      "==================================================\n",
      "==================================================\n",
      "{'instruction': 'Please answer the following MA helpdesk questions:', 'input': \"How to use Cerberus in ma docker I have a custom docker created by the command ma train tf docker for model training. In my training program, I want to query data on hive using queryrunner. The issue is the container doesn't have connections to other services through http port. I tried to tunnel to a production server but get permission denied. What is the proper way to have a connection to the service that I need? Is there a way to use cerberus?\", 'output': \"\\nMost likely your version of the queryrunner-client is out of date and needs to be upgraded.  Using version 1.5.5, the following code  taken from docs here  should be able to execute successfully without Cerberus:from queryrunner_client import Client, start_adhocstart_adhoc    # for galileoqr = Client user_email=userid + '@uber.com' print qr.list_datasources   query = 'select city_id, city_name from dim_city order by city_id asc limit 5;'cursor = qr.execute 'warehouse', query print cursor.executi\", 'text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nPlease answer the following MA helpdesk questions:\\nHow to use Cerberus in ma docker I have a custom docker created by the command ma train tf docker for model training. In my training program, I want to query data on hive using queryrunner. The issue is the container doesn't have connections to other services through http port. I tried to tunnel to a production server but get permission denied. What is the proper way to have a connection to the service that I need? Is there a way to use cerberus?\\n\\n### Response:\\nMost likely your version of the queryrunner-client is out of date and needs to be upgraded.  Using version 1.5.5, the following code  taken from docs here  should be able to execute successfully without Cerberus:from queryrunner_client import Client, start_adhocstart_adhoc    # for galileoqr = Client user_email=userid + '@uber.com' print qr.list_datasources   query = 'select city_id, city_name from dim_city order by city_id asc limit 5;'cursor = qr.execute 'warehouse', query print cursor.executi\\n\"}\n",
      "**************************************************\n",
      "QUESTIONS:\n",
      "How to use Cerberus in ma docker I have a custom docker created by the command ma train tf docker for model training. In my training program, I want to query data on hive using queryrunner. The issue is the container doesn't have connections to other services through http port. I tried to tunnel to a production server but get permission denied. What is the proper way to have a connection to the service that I need? Is there a way to use cerberus?\n",
      "\n",
      "GENERATED:\n",
      "Cerberus is a utility that provides Uber laptop access to production hosts. This tool lives in our internal brew tapset  part of homebrew for Mac packages.To install Cerberus for Linux, see ma cerberus Installation Instructions for Linux.To install Cerberus for Mac:git clone :homebrew-alt  usr local Homebrew-alt  brew install cerberusFor more details, go to https:  code.uberinternal.com w projects code_management uber-homebrew-alt.\n",
      "\n",
      "EXPECTED:\n",
      "\n",
      "Most likely your version of the queryrunner-client is out of date and needs to be upgraded.  Using version 1.5.5, the following code  taken from docs here  should be able to execute successfully without Cerberus:from queryrunner_client import Client, start_adhocstart_adhoc    # for galileoqr = Client user_email=userid + '@uber.com' print qr.list_datasources   query = 'select city_id, city_name from dim_city order by city_id asc limit 5;'cursor = qr.execute 'warehouse', query print cursor.executi\n",
      "==================================================\n",
      "==================================================\n",
      "{'instruction': 'Please answer the following MA helpdesk questions:', 'input': 'Getting asset not found when adding new data source', 'output': '\\nMichelangelo recently changed from ublame to uown and the team name in the project setting may be incorrect.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nPlease answer the following MA helpdesk questions:\\nGetting asset not found when adding new data source\\n\\n### Response:\\nMichelangelo recently changed from ublame to uown and the team name in the project setting may be incorrect.\\n'}\n",
      "**************************************************\n",
      "QUESTIONS:\n",
      "Getting asset not found when adding new data source\n",
      "\n",
      "GENERATED:\n",
      "MA team built MA Studio that is covering the use case of Michelangelo and MA's other services. MA Studio has better ACL with uOwn integration. Please get more information about MAStudio here.With this, we no longer build new feature in the current MLExplorer.\n",
      "\n",
      "EXPECTED:\n",
      "\n",
      "Michelangelo recently changed from ublame to uown and the team name in the project setting may be incorrect.\n",
      "==================================================\n",
      "==================================================\n",
      "{'instruction': 'Please answer the following MA helpdesk questions:', 'input': \"Does Michelangelo DSL have a string concatenation operator I am trying to synthesize features, ideally with string concatenation. However I didn't find one and here is what I am currently doing.day_of_week_X_hour_of_day: nFill if   sFill sVal  @basis:ti_workflowcreatedat_local_day_of_week  ,  NA   ==  Monday    nFill 100 + nVal  @basis:ti_workflowcreatedat_local_hour_of_day  , -1.0   else  if   sFill sVal  @basis:ti_workflowcreatedat_local_day_of_week  ,  NA   ==  Tuesday    nFill 200 + nVal  @b\", 'output': \"\\nOne approach is to create datestr column with fixed value. Something likeCREATE TABLE mydb.t2 ASSELECT  *,   '2019-08-24' as datestrFROM   mydb.t;Afterward, you register another dataset. For above example, mydb.t2. In the model training draft, specify 2019-08-24 to 2019-08-24 for training range.\", 'text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nPlease answer the following MA helpdesk questions:\\nDoes Michelangelo DSL have a string concatenation operator I am trying to synthesize features, ideally with string concatenation. However I didn't find one and here is what I am currently doing.day_of_week_X_hour_of_day: nFill if   sFill sVal  @basis:ti_workflowcreatedat_local_day_of_week  ,  NA   ==  Monday    nFill 100 + nVal  @basis:ti_workflowcreatedat_local_hour_of_day  , -1.0   else  if   sFill sVal  @basis:ti_workflowcreatedat_local_day_of_week  ,  NA   ==  Tuesday    nFill 200 + nVal  @b\\n\\n### Response:\\nOne approach is to create datestr column with fixed value. Something likeCREATE TABLE mydb.t2 ASSELECT  *,   '2019-08-24' as datestrFROM   mydb.t;Afterward, you register another dataset. For above example, mydb.t2. In the model training draft, specify 2019-08-24 to 2019-08-24 for training range.\\n\"}\n",
      "**************************************************\n",
      "QUESTIONS:\n",
      "Does Michelangelo DSL have a string concatenation operator I am trying to synthesize features, ideally with string concatenation. However I didn't find one and here is what I am currently doing.day_of_week_X_hour_of_day: nFill if   sFill sVal  @basis:ti_workflowcreatedat_local_day_of_week  ,  NA   ==  Monday    nFill 100 + nVal  @basis:ti_workflowcreatedat_local_hour_of_day  , -1.0   else  if   sFill sVal  @basis:ti_workflowcreatedat_local_day_of_week  ,  NA   ==  Tuesday    nFill 200 + nVal  @b\n",
      "\n",
      "GENERATED:\n",
      "For each FeatureStore table, we auto register it with Data Stats Service  DSS. DSS collects certain stats on each table and each column. Some of the stats can be found by clicking on Stats view on a specific table under Palette category on Databook.Palette Pipeline FailureWhen you onboard a new feature group, please make sure to add corresponding engineer oncall ldap group and team's ldap group into owner_ldap_groups. Upon failure, an email notification will be sent and the corresponding teams\n",
      "\n",
      "EXPECTED:\n",
      "\n",
      "One approach is to create datestr column with fixed value. Something likeCREATE TABLE mydb.t2 ASSELECT  *,   '2019-08-24' as datestrFROM   mydb.t;Afterward, you register another dataset. For above example, mydb.t2. In the model training draft, specify 2019-08-24 to 2019-08-24 for training range.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "==================================================\n",
      "{'instruction': 'Please answer the following MA helpdesk questions:', 'input': 'yab Michelangelo P2P predication service locally without remote server or cerberus', 'output': \"\\nThanks for the question Jiamin. We're working on a long term solution for cerberus. In the meantime, you can use our tool to start a proxy locally.Upgrade to the latest opsctl versionbrew update  brew install opsctlbrew update  brew upgrade opsctlCreate tunnelopsctl service debug tunnel -p <endpoint-name>:<local-port>This will cause a listener on localhost:<local-port> that will forward to a random remote endpoint instance. Then, your yab request should use --peer localhost:<local-port> instead \", 'text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nPlease answer the following MA helpdesk questions:\\nyab Michelangelo P2P predication service locally without remote server or cerberus\\n\\n### Response:\\nThanks for the question Jiamin. We're working on a long term solution for cerberus. In the meantime, you can use our tool to start a proxy locally.Upgrade to the latest opsctl versionbrew update  brew install opsctlbrew update  brew upgrade opsctlCreate tunnelopsctl service debug tunnel -p <endpoint-name>:<local-port>This will cause a listener on localhost:<local-port> that will forward to a random remote endpoint instance. Then, your yab request should use --peer localhost:<local-port> instead \\n\"}\n",
      "**************************************************\n",
      "QUESTIONS:\n",
      "yab Michelangelo P2P predication service locally without remote server or cerberus\n",
      "\n",
      "GENERATED:\n",
      "Turns out I needed to just install cmake on my machine using brew install cmake\n",
      "\n",
      "EXPECTED:\n",
      "\n",
      "Thanks for the question Jiamin. We're working on a long term solution for cerberus. In the meantime, you can use our tool to start a proxy locally.Upgrade to the latest opsctl versionbrew update  brew install opsctlbrew update  brew upgrade opsctlCreate tunnelopsctl service debug tunnel -p <endpoint-name>:<local-port>This will cause a listener on localhost:<local-port> that will forward to a random remote endpoint instance. Then, your yab request should use --peer localhost:<local-port> instead \n",
      "==================================================\n",
      "==================================================\n",
      "{'instruction': 'Please answer the following MA helpdesk questions:', 'input': 'How to use getFeatures in Palette? any reference implementation or example for getting feature from palette service via Palette::getFeatures?', 'output': '\\nNote: Please be sure that you are using a recent michelangelo-python-client version to ensure that all support for secure cluster use is available.  The most recent version of MPC as of today is 0.6.124.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nPlease answer the following MA helpdesk questions:\\nHow to use getFeatures in Palette? any reference implementation or example for getting feature from palette service via Palette::getFeatures?\\n\\n### Response:\\nNote: Please be sure that you are using a recent michelangelo-python-client version to ensure that all support for secure cluster use is available.  The most recent version of MPC as of today is 0.6.124.\\n'}\n",
      "**************************************************\n",
      "QUESTIONS:\n",
      "How to use getFeatures in Palette? any reference implementation or example for getting feature from palette service via Palette::getFeatures?\n",
      "\n",
      "GENERATED:\n",
      "getFeatures in Palette is a reactive function. You can use callback to get features.\n",
      "\n",
      "EXPECTED:\n",
      "\n",
      "Note: Please be sure that you are using a recent michelangelo-python-client version to ensure that all support for secure cluster use is available.  The most recent version of MPC as of today is 0.6.124.\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for d in test_data:\n",
    "    \n",
    "    instruction = d[\"instruction\"]\n",
    "    input_text= d[\"input\"]\n",
    "    generated = generate_helpdesk_response(instruction, input_text, model, tokenizer)\n",
    "    expected = d['output']\n",
    "    print(\"=\"*100)\n",
    "    print(\"=\"*100)\n",
    "    print(d)\n",
    "    print(\"*\"*100)\n",
    "    print(\"QUESTIONS:\")\n",
    "    print(input_text)\n",
    "    print(\"\\nGENERATED:\")\n",
    "    print(generated)\n",
    "    print(\"\\nEXPECTED:\")\n",
    "    print(expected)\n",
    "    count += 1\n",
    "    if count > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d949925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTIONS: \n",
      "\n",
      "Our ml-code devpod's drogon-cli is too old and running drogon command from it will produce error:\n",
      "got Unexpected response from uSSO - {\"version\":\"1.0.0\",\"type\":\"server side error\",\"message\":\"invalid response from Duo statuscode: 400\",\"error\":true}\n",
      "\n",
      "GENERATED:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Golang doesn't have a nice support for PyML's Arrow encoding yet. The simplest way to support unicode would be just encoding unicode strings with base64 and then decoding them at the model level upon receiving.We can look into golang client support of arrow encoding to make this even more simpler  the same way it works for python.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"Please answer the following MA helpdesk questions:\"\n",
    "input_text = \"\"\"Our ml-code devpod's drogon-cli is too old and running drogon command from it will produce error:\n",
    "got Unexpected response from uSSO - {\"version\":\"1.0.0\",\"type\":\"server side error\",\"message\":\"invalid response from Duo statuscode: 400\",\"error\":true}\"\"\"\n",
    "output_text=\"\"\n",
    "print(\"QUESTIONS: \\n\")\n",
    "print(input_text)\n",
    "print(\"\\nGENERATED:\")\n",
    "generate_helpdesk_response(instruction, input_text, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa640f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

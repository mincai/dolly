{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cc0ddec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/bo_ling/python39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from training.generate import (generate_response, load_model_tokenizer_for_generate, \n",
    "                               get_special_token_id, get_special_token_id)\n",
    "from training.consts import END_KEY, PROMPT_FORMAT, RESPONSE_KEY_NL\n",
    "from training.trainer import PROMPT_FORMAT, create_data_set_from_json_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bdb9ac5",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Please make sure that the input data file be in /home/bo_ling/dataset/michelangelo_so_long.jsonl,/home/bo_ling/dataset/docstrans.jsonl,/home/bo_ling/dataset/eats_prod.jsonl",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_data_set_from_json_list\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/opt/home/bo_ling/dolly/ma_data/so3_long.jsonl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m train_data\n",
      "File \u001b[0;32m/opt/home/bo_ling/dolly/training/trainer.py:64\u001b[0m, in \u001b[0;36mcreate_data_set_from_json_list\u001b[0;34m(json_list_file, max_question_length, max_answer_length)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m json_list_file \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m FILE_TO_INSTRUCTION_MAP:\n\u001b[1;32m     63\u001b[0m     all_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(FILE_TO_INSTRUCTION_MAP\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease make sure that the input data file be in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mall_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgen\u001b[39m():\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(json_list_file) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[0;31mException\u001b[0m: Please make sure that the input data file be in /home/bo_ling/dataset/michelangelo_so_long.jsonl,/home/bo_ling/dataset/docstrans.jsonl,/home/bo_ling/dataset/eats_prod.jsonl"
     ]
    }
   ],
   "source": [
    "train_data = create_data_set_from_json_list(\"/opt/home/bo_ling/dolly/ma_data/so3_long.jsonl\")\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d1b0ebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_output_dir=\"/opt/home/bo_ling/dolly_training/ma_helpdesk_2048/checkpoint-800\"\n",
    "model, tokenizer = load_model_tokenizer_for_generate(local_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "61f88a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_helpdesk_response(\n",
    "    instruction: str,\n",
    "    input_text: str,\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    do_sample: bool = True,\n",
    "    max_new_tokens: int = 256,\n",
    "    top_p: float = 0.92,\n",
    "    top_k: int = 0,\n",
    "    **kwargs,\n",
    ") -> str:\n",
    "    input_ids = tokenizer(PROMPT_FORMAT.format(instruction=instruction, input_text=input_text, output_text=\"\"),\n",
    "                          return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "    response_key_token_id = get_special_token_id(tokenizer, RESPONSE_KEY_NL)\n",
    "    end_key_token_id = get_special_token_id(tokenizer, END_KEY)\n",
    "\n",
    "    gen_tokens = model.generate(\n",
    "        input_ids,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        # Ensure generation stops once it generates \"### End\"\n",
    "        eos_token_id=end_key_token_id,\n",
    "        do_sample=do_sample,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        **kwargs,\n",
    "    )[0].cpu()\n",
    "\n",
    "    # The response will be set to this variable if we can identify it.\n",
    "    decoded = None\n",
    "\n",
    "    # Find where \"### Response:\" is first found in the generated tokens.  Considering this is part of the prompt,\n",
    "    # we should definitely find it.  We will return the tokens found after this token.\n",
    "    response_pos = None\n",
    "    response_positions = np.where(gen_tokens == response_key_token_id)[0]\n",
    "    if len(response_positions) == 0:\n",
    "        logger.warn(f\"Could not find response key {response_key_token_id} in: {gen_tokens}\")\n",
    "    else:\n",
    "        response_pos = response_positions[0]\n",
    "\n",
    "    if response_pos:\n",
    "        # Next find where \"### End\" is located.  The model has been trained to end its responses with this sequence\n",
    "        # (or actually, the token ID it maps to, since it is a special token).  We may not find this token, as the\n",
    "        # response could be truncated.  If we don't find it then just return everything to the end.  Note that\n",
    "        # even though we set eos_token_id, we still see the this token at the end.\n",
    "        end_pos = None\n",
    "        end_positions = np.where(gen_tokens == end_key_token_id)[0]\n",
    "        if len(end_positions) > 0:\n",
    "            end_pos = end_positions[0]\n",
    "\n",
    "        decoded = tokenizer.decode(gen_tokens[response_pos + 1 : end_pos]).strip()\n",
    "\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1e1c846c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "====================================================================================================\n",
      "QUESTIONS:\n",
      "Unable to build michelangelo core\n",
      "\n",
      "GENERATED:\n",
      "You need protobuf 2.5.0.   Documents workspace michelangelo michelangelo online-prediction   master  protoc --versionlibprotoc 2.5.0Also make sure you are connected to the VPN\n",
      "\n",
      "EXPECTED:\n",
      "\n",
      "You need protobuf 2.5.0.   Documents workspace michelangelo michelangelo online-prediction   master  protoc --versionlibprotoc 2.5.0Also make sure you are connected to the VPN\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "QUESTIONS:\n",
      "Caffe training failed with message -- KeyError: Unable to open object  Object 'data' doesn't exist \n",
      "\n",
      "GENERATED:\n",
      "You might hit this error when you train a Caffe deep learning model from michelangelo.Most likely your data is not image data and the draft model has a wrong configuration so that above code path  sampling image data  got executed which is wrong.To fix it, check  skip activation data  from the draft model dialog, then re-train your model.\n",
      "\n",
      "EXPECTED:\n",
      "\n",
      "You might hit this error when you train a Caffe deep learning model from michelangelo.Most likely your data is not image data and the draft model has a wrong configuration so that above code path  sampling image data  got executed which is wrong.To fix it, check  skip activation data  from the draft model dialog, then re-train your model.\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "QUESTIONS:\n",
      "rename copy a michelangelo project\n",
      "\n",
      "GENERATED:\n",
      "You might hit this error when you train a Caffe deep learning model from michelangelo.Most likely your data is not image data and the draft model has a wrong configuration so that above code path  sampling image data  got executed which is wrong.To fix it, check  skip activation data  from the draft model dialog, then re-train your model.\n",
      "\n",
      "EXPECTED:\n",
      "\n",
      "You might hit this error when you train a Caffe deep learning model from michelangelo.Most likely your data is not image data and the draft model has a wrong configuration so that above code path  sampling image data  got executed which is wrong.To fix it, check  skip activation data  from the draft model dialog, then re-train your model.\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "QUESTIONS:\n",
      "Spark Driver was killed by YARN\n",
      "\n",
      "GENERATED:\n",
      "The problem is resolved now. At that time it was likely due to a hadoop outage\n",
      "\n",
      "EXPECTED:\n",
      "\n",
      "The problem is resolved now. At that time it was likely due to a hadoop outage\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "QUESTIONS:\n",
      "Changing project name in Michelangelo We have a project named  etaFit  but we'd like to rename it to  ufpFit  because it's a more descriptive name of what the project is used for. Is there a way to do this  just once  from the Michelangelo side, or should I just create a new project?\n",
      "\n",
      "GENERATED:\n",
      "You can change the project name in the settings view.\n",
      "\n",
      "EXPECTED:\n",
      "\n",
      "Sorry, this is not supported. You'll have to create a new project.\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "QUESTIONS:\n",
      "Unexpected token `>` when running michelangelo-web\n",
      "\n",
      "GENERATED:\n",
      "Check the node version in the error log:npm ERR! node v0.10.32npm ERR! npm  v2.8.2michelangelo-web now keeps the node version updated in package.json, so you will need the correct version of node to run it. You can use @uber change-node to make this easy.npm install -g @uber change-nodethen in the working directory run:cnin the working directory$ cnnode v6.10.0  from package.json engines node  already installednpm v4.0.5  from package.json engines npm  already installed\n",
      "\n",
      "EXPECTED:\n",
      "\n",
      "Check the node version in the error log:npm ERR! node v0.10.32npm ERR! npm  v2.8.2michelangelo-web now keeps the node version updated in package.json, so you will need the correct version of node to run it. You can use @uber change-node to make this easy.npm install -g @uber change-nodethen in the working directory run:cnin the working directory$ cnnode v6.10.0  from package.json engines node  already installednpm v4.0.5  from package.json engines npm  already installed\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for d in train_data:\n",
    "    instruction = d[\"instruction\"]\n",
    "    input_text= d[\"input\"]\n",
    "    generated = generate_helpdesk_response(instruction, input_text, model, tokenizer)\n",
    "    expected = d['output']\n",
    "    print(\"=\"*100)\n",
    "    print(\"=\"*100)\n",
    "    print(\"QUESTIONS:\")\n",
    "    print(input_text)\n",
    "    print(\"\\nGENERATED:\")\n",
    "    print(generated)\n",
    "    print(\"\\nEXPECTED:\")\n",
    "    print(expected)\n",
    "    count += 1\n",
    "    if count > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7d949925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTIONS: \n",
      "\n",
      "Our ml-code devpod's drogon-cli is too old and running drogon command from it will produce error:\n",
      "got Unexpected response from uSSO - {\"version\":\"1.0.0\",\"type\":\"server side error\",\"message\":\"invalid response from Duo statuscode: 400\",\"error\":true}\n",
      "\n",
      "GENERATED:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"We have deprecated the JSON + HTTP endpoint from our API, and replaced them as THRIFT + HTTP endpoint.The thrift version of this endpoint is this onehttps:  engdocs.uberinternal.com michelangelo-rest thrift michelangelo_api.html#Fn_MichelangeloAPI_getGroupedDeploymentStatusesHere is the function wrapper of that endpoint in piper-core-pipelines repo.https:  code.uberinternal.com diffusion DAPIPE browse master michelangelo_platform lib rest_api.py$300We don't recommend people directly calling the\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"Answer the following MA helpdesk questions:\"\n",
    "input_text = \"\"\"Our ml-code devpod's drogon-cli is too old and running drogon command from it will produce error:\n",
    "got Unexpected response from uSSO - {\"version\":\"1.0.0\",\"type\":\"server side error\",\"message\":\"invalid response from Duo statuscode: 400\",\"error\":true}\"\"\"\n",
    "output_text=\"\"\n",
    "print(\"QUESTIONS: \\n\")\n",
    "print(input_text)\n",
    "print(\"\\nGENERATED:\")\n",
    "generate_helpdesk_response(instruction, input_text, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa640f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df372c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "SCRIPT_DIR = os.path.dirname(os.path.abspath(\"...\"))\n",
    "sys.path.append(os.path.dirname(SCRIPT_DIR))\n",
    "\n",
    "from training.agent_trainer import END_KEY, RESPONSE_KEY_NL, CHAT_START_KEY\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24f3fbfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 16958\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_data = load_from_disk(\"/opt/home/bo_ling/dataset/modeling_data_v1.hf\")['test']\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e560587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_output_dir=\"/opt/home/agent_modeling_data_v1_checkpoint-60000\"\n",
    "#model, tokenizer = load_model_tokenizer_for_generate(local_output_dir)\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_output_dir, padding_side=\"left\")\n",
    "device = \"cpu\" # \"cuda:3\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForCausalLM.from_pretrained(local_output_dir, trust_remote_code=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eb0b69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endofsentence|> [50400]\n"
     ]
    }
   ],
   "source": [
    "eos_token_id = tokenizer.encode(END_KEY)\n",
    "print(END_KEY, eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a3c253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_agent_response(\n",
    "    texts: str,\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    do_sample: bool = True,\n",
    "    max_new_tokens: int = 256,\n",
    "    top_p: float = 0.92,\n",
    "    top_k: int = 0,\n",
    "    **kwargs,\n",
    ") -> str:\n",
    "    #texts = texts.replace(END_KEY, \"\")\n",
    "    input_ids = tokenizer(texts, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    response_key_token_id = tokenizer.encode(RESPONSE_KEY_NL)[0]\n",
    "    end_key_token_id = tokenizer.encode(END_KEY)[0]\n",
    "    gen_tokens = model.generate(\n",
    "        input_ids,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        # Ensure generation stops once it generates \"### End\"\n",
    "        eos_token_id=end_key_token_id,\n",
    "        do_sample=do_sample,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        **kwargs,\n",
    "    )[0].cpu()\n",
    "\n",
    "    # The response will be set to this variable if we can identify it.\n",
    "    question_size = len(input_ids[0])\n",
    "    decoded = tokenizer.decode(gen_tokens[question_size:]).strip()\n",
    "\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3636b3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_generated_chats(chat_str):\n",
    "    chats = chat_str.replace(\"\\n\\n###\\n\", END_KEY).split(END_KEY)\n",
    "    curr_text = chats[0] + END_KEY\n",
    "    print(chats[0])\n",
    "    for chat in chats[1:]:\n",
    "        print(chat)\n",
    "        if chat.startswith(RESPONSE_KEY_NL):\n",
    "            generated = generate_agent_response(curr_text, model, tokenizer).strip(END_KEY)\n",
    "            if generated.replace(\"\\n\", \"\").startswith(RESPONSE_KEY_NL.replace(\"\\n\", \"\")):\n",
    "                print(\"\\n\")\n",
    "                print(\"*\"*40 + \"START OUTPUT GENERATED\" + \"*\"*40)\n",
    "                print(generated)\n",
    "                print(\"*\"*40 + \"END OUTPUT GENERATED\" + \"*\"*40)\n",
    "                print(\"\\n\")\n",
    "        curr_text += chat + END_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a4709e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specific Information: The restaurant name is Salt & Straw (Capitol Hill) and the order value is $29.36. The estimated delivery time is 18:38 and latest arrival time is 18:45. Order status is 3.\n",
      "\n",
      "agent: Hi Kaye, welcome to Chat Support team. Thanks for being a member! I will be happy to assist you today. \n",
      "\n",
      "agent: I'm sorry to hear that your order is taking time longer than expected. I understand that your delivery time keeps changing. Allow me a moment while I am checking your order details. \n",
      "\n",
      "\n",
      "****************************************START OUTPUT GENERATED****************************************\n",
      "agent: I can understand your concern, Kaye that your order is taking too long to arrive. I am sorry for the inconvenience caused to you. Please be with me, while I am assisting with your concern. \n",
      "****************************************END OUTPUT GENERATED****************************************\n",
      "\n",
      "\n",
      "\n",
      "Kaye: Thanks Pammy \n",
      "\n",
      "agent: I have reviewed the status of your order and can see that your delivery person is on their way to drop off your order. \n",
      "\n",
      "\n",
      "****************************************START OUTPUT GENERATED****************************************\n",
      "agent: I have reviewed the status of your order and can see that your delivery person is on their way to drop off your order. \n",
      "****************************************END OUTPUT GENERATED****************************************\n",
      "\n",
      "\n",
      "\n",
      "agent: Kaye, we do our best to fulfill every order on time, but sometimes delays do happen due to unforeseen circumstances. Please note that this is our best estimate. Actual delivery times may fluctuate depending on how busy the merchant is, how large your order is, traffic conditions, and other factors. \n",
      "\n",
      "\n",
      "****************************************START OUTPUT GENERATED****************************************\n",
      "agent: Kaye, we do our best to fulfill every order on time, but sometimes delays do happen due to unforeseen circumstances. Please note that this is our best estimate. Actual delivery times may fluctuate depending on how busy the merchant is, how large your order is, traffic conditions, and other factors. \n",
      "****************************************END OUTPUT GENERATED****************************************\n",
      "\n",
      "\n",
      "\n",
      "Kaye: I see \n",
      "\n",
      "agent: I have highlighted this issue for you Kaye, don't worry, I will try to make sure you get the order as soon as possible. \n",
      "\n",
      "agent: If there is anything else that I can further assist you with, please do let me know. \n",
      "\n",
      "\n",
      "****************************************START OUTPUT GENERATED****************************************\n",
      "agent: If there is anything else that I can further assist you with, please do let me know. \n",
      "****************************************END OUTPUT GENERATED****************************************\n",
      "\n",
      "\n",
      "\n",
      "agent: Thank you for chatting with us today. Stay safe and stay healthy. \n",
      "\n",
      "\n",
      "****************************************START OUTPUT GENERATED****************************************\n",
      "agent: Thank you for chatting with us today. Stay safe and stay healthy. \n",
      "****************************************END OUTPUT GENERATED****************************************\n",
      "\n",
      "\n",
      "\n",
      "agent: Have we resolved this issue? \n",
      "\n",
      "\n",
      "****************************************START OUTPUT GENERATED****************************************\n",
      "agent: Yes No \n",
      "****************************************END OUTPUT GENERATED****************************************\n",
      "\n",
      "\n",
      "\n",
      "agent: Yes No \n",
      "\n",
      "\n",
      "****************************************START OUTPUT GENERATED****************************************\n",
      "agent: Yes No \n",
      "****************************************END OUTPUT GENERATED****************************************\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "chat_str = test_data[i]['text']\n",
    "print_generated_chats(chat_str)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "20a9e52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specific Information: The restaurant name is Chuy's (4544 McKinney Ave.) and the order value is $18.46. The estimated delivery time is 21:55 and latest arrival time is 21:50. Order status is 5.\n",
      "\n",
      "agent: Hi Krystal. Welcome to Chat Support team. I will be assisting you with your concern. \n",
      "\n",
      "agent: \"Sorry to hear about the inconvenience\" \n",
      "\n",
      "\n",
      "****************************************START OUTPUT GENERATED****************************************\n",
      "agent: Krystal, what I understand so far is that your order is getting delayed for delivery, right? \n",
      "****************************************END OUTPUT GENERATED****************************************\n",
      "\n",
      "\n",
      "\n",
      "agent: I have checked your order details and I can confirm that a delivery person named Eric is currently en route to pick up your order. \n",
      "\n",
      "\n",
      "****************************************START OUTPUT GENERATED****************************************\n",
      "agent: Ive looked into your order and it looks like a delivery person is on their way to pick up your order. \n",
      "****************************************END OUTPUT GENERATED****************************************\n",
      "\n",
      "\n",
      "\n",
      "agent: If there's anything else I can help you with, please let me know. \n",
      "\n",
      "\n",
      "****************************************START OUTPUT GENERATED****************************************\n",
      "agent: If there's anything else I can help you with, please let me know. \n",
      "****************************************END OUTPUT GENERATED****************************************\n",
      "\n",
      "\n",
      "\n",
      "agent: As I have not heard back from you in a while, I am closing the chat. If you continue to experience this issue, feel free to start another chat and a member of our team will be happy to help. \n",
      "\n",
      "\n",
      "****************************************START OUTPUT GENERATED****************************************\n",
      "agent: Have we resolved this issue? \n",
      "****************************************END OUTPUT GENERATED****************************************\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "chat_str = test_data[i]['text']\n",
    "print_generated_chats(chat_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32218463",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
